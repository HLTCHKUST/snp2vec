{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import pysam\n",
    "import time\n",
    "import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import json\n",
    "from tokenizers import (ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt_path = './pretraining_data/token_NC_000019.9.txt' # Generated file from `sample_chromosome_matrix.py`\n",
    "output_txt_path = './pretraining_data/pretraining_data_ch19.txt' # Output pretraining dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt_biallele_code = json.load(open('./resource/snp_vocab.json', 'r'))\n",
    "nt_to_index = json.load(open('./resource/nucleotide_to_index.json', 'r'))\n",
    "index_to_nt = {v:k for k,v in nt_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Remove leading and ending 'N' tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_NC_000019.9.txt | len(seq) 59128983 | s_idx 60000 | e_idx 59118982 | len(trim_seq) 59058983\n"
     ]
    }
   ],
   "source": [
    "n_token_code = nt_biallele_code['N']\n",
    "seq = open(input_txt_path, 'rb').read().decode('utf-8')\n",
    "\n",
    "s_idx = -1\n",
    "for i in range(len(seq)):\n",
    "    if seq[i] != n_token_code:\n",
    "        s_idx = i\n",
    "        break\n",
    "\n",
    "e_idx = -1\n",
    "for i in range(len(seq)-1,0,-1):\n",
    "    if seq[i] != n_token_code:\n",
    "        e_idx = i\n",
    "        break            \n",
    "\n",
    "# Trim sequence\n",
    "trim_seq = seq[s_idx:e_idx+1]\n",
    "print(f'{txt_path.split(\"/\")[-1]} | len(seq) {len(seq)} | s_idx {s_idx} | e_idx {e_idx} | len(trim_seq) {len(trim_seq)}')\n",
    "\n",
    "# Save to file\n",
    "prefix_path = '/'.join(txt_path.split(\"/\")[:-1])\n",
    "out_path = f'{prefix_path}/clean_{txt_path.split(\"/\")[-1]}'\n",
    "out_file = open(out_path, 'wb')\n",
    "out_file.write(trim_seq.encode('utf8'))\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences from Chromosome 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 50\n",
    "    \n",
    "possible_start_idxs = list(range(4096))\n",
    "possible_segment_lens = list(range(512, 4096))\n",
    "\n",
    "clean_txt_path = f'{prefix_path}/clean_{txt_path.split(\"/\")[-1]}'\n",
    "out_file = open(output_txt_path, 'wb')\n",
    "\n",
    "seq = open(clean_txt_path, 'rb').read().decode('utf-8')\n",
    "start_idxs = np.random.choice(possible_start_idxs, num_iterations, replace=False)\n",
    "\n",
    "for start_idx in start_idxs:\n",
    "    s_idx = start_idx\n",
    "    while s_idx < ch19_len_clean:\n",
    "        segment_len = np.random.choice(possible_segment_lens)\n",
    "        segment = seq[s_idx:s_idx+segment_len]\n",
    "\n",
    "        out_file.write((segment + '\\n').encode('utf8'))\n",
    "        s_idx = s_idx + segment_len\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build BPE tokenizer vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = open(output_txt_path, 'r', encoding='utf-8').read().split('\\n')\n",
    "unigram_tokens = list(nt_biallele_code.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizers/chr19_diploid-vocab.json',\n",
       " './tokenizers/chr19_diploid-merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(seqs)\n",
    "\n",
    "char_bpe_tokenizer = SentencePieceBPETokenizer()\n",
    "max_num_sentences = 3000000\n",
    "\n",
    "char_bpe_tokenizer.train_from_iterator(seqs[:max_num_sentences], special_tokens=['[UNK]','[CLS]','[SEP]'], vocab_size=32000, initial_alphabet=unigram_tokens)\n",
    "char_bpe_tokenizer.save_model(directory='./tokenizers/', prefix='chr19_diploid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bpe_tokenizer = SentencePieceBPETokenizer(\n",
    "    vocab='./tokenizers/chr19_diploid-vocab.json',\n",
    "    merges='./tokenizers/chr19_diploid-merges.txt',\n",
    "    unk_token='[UNK]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'TAGAG', 'ACTT', 'AAGTGACC', 'ACCCC', 'AGGGCTG', 'CCG', 'TTGCTCAGG', 'TGTGTT', 'TCTGG', 'CATTCC', 'CAAGTTGG', 'TACCCTT', 'ACATGCAA', 'TATTTT', 'ACATT', 'AGAATG', 'CATG', 'CGTTTG', 'AAATAG', 'CGAA', 'TTGGTGAA', 'CATGTAA', 'ACCAGATG', 'CTAAG', 'AGGGCTTTG', 'AGAG', 'CT', 'AGCCTGGG', 'CACC', 'CAATT', 'ACA', '锕', 'AAGGGG', 'CATT', 'ATTTCC', 'ACAG', 'CAAAG', 'CATATT', 'TCAGGGTCC', 'ATACAG', 'CAA', 'ACAAGG', 'TCAAATCTG', 'AAAG', 'ATGGGG', 'TCTGCAA', 'ACCTCC', 'ATCTT', 'CTTTG', 'TAGTGATG', 'TTTTAATCC', 'CATTCC', 'TCTCCAA', 'TCCTAG', 'CTCAAGAA', 'TCTGTTAG', 'TAAGAG', 'CATGGGGG', 'AGAATG', 'ATGAGAAG', 'CCTGGTT', 'TAAAAAACAA', 'ACACATCAA', 'ATTCTCTG', 'CTACC', 'ACTT', 'ATGAGTT', 'CTGTGACTT', 'TAGTCC', 'TTTG', 'CTTGGAA', 'TCTTCC', 'AGAA', 'TCATACC', 'CTAAG', 'ATAAGG', 'ATTTAGG', 'CACAGGCAG', 'TTTTTT', 'GGGAGG', 'AGGG', 'AGAGAAG', 'ATCC', 'CAGGAAG', 'TACAAGG', 'AGGGAGTGG', 'GGGG', 'AGCAG', 'ATGTAGG', 'CGTG', 'TTTTG', 'AAGCAGG', 'TGATCC', 'CTG', 'TGGGAA', 'ATGGAG', 'TCTTATT', 'CCTG', 'CTGGGACC', 'CTCCG', 'AGGAA', 'CTATGTGG', 'ACAACC', 'TCAAGAAAG', 'CTGCGG', 'TGTTTAT', 'AAGCC', 'AGCCCC', 'TATCCTG', 'TCTTGG', 'TTGAGTG', 'TATT', 'CCTGGGG', 'TGTT', 'CACCCTT', 'TGGCC', 'CTTTAA', 'TCCAA', 'TAAG', 'AGCCCATG', 'ACTGG', 'AGAACCCC', 'CTTGGCC', 'AGAAAGCC', 'CCATATG', 'CTTTGG', 'CATATG', 'TGGAAACAG', 'CCTGCAGG', 'AGATGG', 'CCCAGG', 'GGAA', 'TGGGTGAGG', 'ACTCC', 'AGTGCC', 'CCTG', 'TCACATG', 'AGAATCG', 'AGCC', 'TCTCCG', 'AGCCTCC', '哥', 'TTTCC', 'TCATCTG', 'CACTGTCAG', 'CTGTTCC', 'AGCCCGCC', 'TAGG', 'ACCCTT', 'C']\n",
      "['▁TG', 'CTGG', 'ATGCTG', 'AGGGACA', 'CATCAAG', 'ATCACTG', 'ACTT', '塔GG', 'CATGTG', 'TAAGG', 'AGAACG', 'TCTTCCCC', 'GGG', 'ACG', 'ACAACC', 'CGCACC', 'TTCTG', 'CGGG', 'ACCCCGG', 'ACT', 'ACATAG', 'CCCCGG', 'AGGTAA', 'CCCCAACC', 'CTGCTG', 'CTCTGG', 'TCACG', 'CTTTGAG', 'ATCC', 'CTTAG', 'AGGGTG', 'TAGCTG', 'ATGG', 'TCCAG', 'TATTCACC', 'ACGGG', 'TGAGG', 'CCTGACC', 'CTCAG', 'ACCTTG', 'TCATGAG', 'TTGTGG', 'CCTTCTT', 'ACACAGCCAG', '她', 'CGTT', 'CCTCCAGCC', 'TCCAG', 'CACAGG', 'TGAG', 'CTTGG', 'CACTGAG', 'CCTGCCAGG', 'TGGGCCCAG', 'CTGGG', 'TCTCTAA', 'ATAGG', 'TAAGG', 'TGGGCAG', 'CACCTG', 'TGGG', 'TGAATG', 'TTCCAGG', 'AGAGTGGG', 'ACCAG', 'CTCG', 'TAGGAA', 'TTCCAAG', 'TAGG', 'ACCTG', 'ACCCTGG', 'ATCCTTCTG', 'AGAAGGGG', 'CAGACG', 'ATTTCT', 'AGTG', 'TACT', 'CTGAG', 'TGGGTGTGG', '嚓', 'CTGTCC', 'CCTGCCAA', 'CACTGAA', 'CATG', 'TCCGG', 'ACTATCTT', 'CTGAA', 'TACTT', 'TAA', 'ACTGGG', 'CAGGG', 'CTCTCC', 'CTGGAG', 'TATTCAG', 'TTGGATGG', 'AAGCTT', 'ATTTCC', 'TGTGTTG', 'TACG', 'TGTT', 'TCTCTG', 'AT糕', 'TAAGTG', '个', 'ACTGG', 'ACTT', 'CTGTGCTG', 'CATTTT', 'TCAAG', 'AGGGCAGG', 'ATCAG', 'CTGGGCGCGGTGGCTCACACCTGTAATCCCAG', 'CACTTTGGGAGGCTGAGGCAGGTGG', 'ATCACTTGAGGTCAGGAGTTTGAGACC', 'AGCCTGGCCAACATGGTGAAACC', 'TCATCT', 'CTAA', 'CAAAATT', 'ACACAAATT', 'AGCCGGGCGTGGTGG', 'CATG', 'CGCCTG']\n",
      "['▁G', 'ATTTT', 'CAAGG', 'AAAGG', 'TAAAGG', 'ACT', 'CCCCCG', 'ATGCCTTG', 'CAGTT', 'AGTGACTT', 'CAAAATG', 'TTTGCC', 'CGAA', 'TTGGGG', 'TTGGGG', 'ATGGGG', '塔', 'CGGCGG', 'CAAGTAGG', 'GGAG', 'CACGG', 'AGGAA', 'ATCCTTTG', 'CTGTG', 'TAAAAG', 'CATTG', 'CTTTT', 'CTTTTG', 'CATCC', 'TTTTTT', 'CTCCTTCC', 'CAA', 'TTGGTTTT', 'TCAGATT', 'AAGG', 'CTTTTATTTT', 'CAACGTGG', 'ACC', 'TTTTGG', '哥', 'CGAGTG', 'TTTTG', 'TCCCG', 'ATCTTG', 'AAG', 'ATACC', '锕G', 'TCTTG', '锕', 'TTTTTTTTTTTTTTTTTTTT', '个G', 'T礤', 'AGTG', '锕', 'CTCC', 'TTGGGG', '阿', 'AAGTAG', 'CAAGGG', '阿', 'ATAAG', 'CGG', 'AAAAG', 'CCG', 'CTTCG', 'CAGGAA', 'TTTGAAG', 'CAAGTTTT', 'CAGTTTGG', 'ATTG', 'ATAGGCTG', 'CATTCAAG', 'CTTGATT', 'CCTGG', 'TGCTG', 'TCAGTCA', 'CTTAG', 'CACACTG', 'TCACTT', 'TAGGGAAG', 'TGGCTT', 'AACC', 'TCTCTG', 'AACC', 'TCTTAG', 'TTTT', 'CCTCCTT', 'TGTGAAAG', 'CTCCTT', 'TGTG', 'AAAGGGGG', 'TAA', 'TCTCCCAA', 'ATGGAA', 'TTGGTGAA', 'AGCCTG', 'TAAAATGG', 'TTCCTCG', 'CAAG', 'CGGGAAG', 'CACTATTG', 'TAATTG', 'TCTGG', 'TCCAA', 'TCTCC', 'CTCATTGTT', 'CAGCAG', 'ATAA', 'CTTCTTG', 'AGGCC', 'CTACT', 'AAGTG', 'TTGGATGG', 'TCGGGGG', 'TTGG', 'ATCCTGG', 'AGCC', 'TCTGCAG', 'TCTTGGAG']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(char_bpe_tokenizer.encode(seqs[i]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
